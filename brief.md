# Experiment Design: Feature-Space Data Compression for Image Classification

## Background & Motivation

Data compression for model training (often called *dataset distillation*) aims to condense a large dataset into a much smaller set of highly informative samples[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=capacity ,26%2C 2 %2C  28). The goal is that a model trained on this compact "distilled" data achieves similar performance to one trained on the full dataset[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=capacity ,26%2C 2 %2C  28). This concept was first introduced by Wang et al. and has since seen many improvements[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=Dataset distillation was first introduced,45%2C 20 %2C  47). Early methods treated the distilled samples as learnable parameters in a meta-learning framework or matched network gradients to optimize synthetic data[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=Dataset distillation was first introduced,45%2C 20 %2C  47). Recent approaches even use generative models (e.g. diffusion models) to synthesize data in a latent feature space, which speeds up distillation and improves performance[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=However%2C these methods are compute,Compared to).

However, most existing works assume a static dataset and focus on compressing all classes at once. They rarely consider **incremental learning** – i.e. adding new classes or data after the initial compression. In practice, if one already has the original dataset and a high-performance model trained on it, it may seem unnecessary to train a new model on compressed data (especially if the compressed-data model’s performance is slightly worse). The real utility of data compression would be in scenarios like:

- **Incremental Learning:** When new classes or samples arrive over time, we want to integrate them without retraining from scratch or storing the entire original dataset. Existing incremental learning methods often keep a small set of exemplars from old classes to avoid forgetting[cs.jhu.edu](https://www.cs.jhu.edu/~yyliu/preprints/Class_Incremental_Exemplar_Compression_for_Class_Incremental_Learning.pdf#:~:text=Class,18%2C 37%2C 42%2C 45%2C 46). Some works have even explored compressing or optimizing these exemplars for better retention[cs.jhu.edu](https://www.cs.jhu.edu/~yyliu/preprints/Class_Incremental_Exemplar_Compression_for_Class_Incremental_Learning.pdf#:~:text=The replay,these two works in three). If we can compress each class’s data into a few high-density **prototypes**, adding new classes becomes as simple as generating new prototypes and updating the model with minimal overhead.
- **Resource Constraints:** Training on the full dataset for many epochs is time-consuming and compute-intensive. If a compact feature-space representation can be learned, a model might reach high accuracy in 1-2 epochs using the compressed data, instead of N epochs on the original data. This greatly speeds up experimentation and deployment.
- **Storage and Privacy:** Storing or sharing large datasets is costly and can raise privacy/copyright issues. A distilled dataset (especially in feature space) can be much smaller and less identifiable[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=distilling information from large datasets,7 %2C  27%2C 15), making it easier to store or share without exposing raw images.

In summary, **the idea is feasible** and aligns with trends in dataset distillation and few-shot learning. The key novelty here is focusing on compressing *features* using a strong pre-trained model (e.g. CLIP) and designing the system to naturally support incremental addition of new classes. Next, we outline an experiment to validate this idea.

## Proposed Approach

The approach breaks the model into two parts: (1) a **feature projection module** that compresses raw data into a shallow, generalizable feature space, and (2) a **task-specific module** (classifier head) that learns to perform classification using those compressed features. We will use a large pre-trained vision model (CLIP’s image encoder) as the feature extractor and then learn a lightweight compression mapping. Key components:

- **Pre-trained Feature Extractor (CLIP):** We use CLIP’s visual encoder (e.g. a ResNet or ViT backbone) to project each input image into a high-dimensional feature space. CLIP is kept **frozen** so that its rich visual representations (which combine image and semantic knowledge) serve as a stable input. Each image, after CLIP, might be represented by a feature map (e.g. a 7×7×1024 tensor for a ResNet, or a 512-D feature vector if we use the global pooled embedding).
- **Feature Compression Mapping:** Next, we introduce a simple learnable mapping (a shallow neural network, e.g. a **1×1 convolution layer** or small fully-connected layer) to compress CLIP’s output features into a much smaller set of **compressed features**. This mapping is the crux of our “data compression.” For instance, the conv layer might reduce a 1024-channel CLIP feature map down to *n* channels (where *n* is much smaller, like 10 or 50). Each output channel of this conv can be thought of as capturing a certain important feature or prototype. The mapping is trained while CLIP is frozen. Because it’s a 1×1 conv, each kernel learns to extract a particular salient feature combination across the CLIP feature dimensions, effectively acting like a prototype detector.
  - **Interpretation:** If we allocate one or a few output channels per class, the conv could learn class-specific feature maps. For example, channel 1 might respond strongly to features of class “dog,” channel 2 to “cat,” etc. In practice we won’t manually assign channels to classes, but the training will naturally organize the compressed features to help classification. The output of this projection is a **shallow feature space** that ideally retains class-discriminative information but with far lower dimensionality than the original CLIP features.
- **Classifier Head:** Finally, a classification model (which could be as simple as a global average pooling followed by a linear layer/softmax, or a tiny CNN detection head if doing detection) is trained on these compressed features. This corresponds to the second part of the split network, which operates purely in the learned low-dimensional feature space. Because the features are meant to be highly informative, the classifier should be able to learn the task with minimal training epochs.
- **Incremental Learning Mechanism:** To allow adding new classes without perturbing existing ones, we plan to **incrementally train** the mapping and classifier:
  - We start with an initial set of classes, train the mapping conv and classifier on those.
  - When a new class arrives, we **freeze** or lightly fine-tune the existing mapping for old classes and **add new parameters** (e.g. add one new conv filter for the new class, and a new output node in the classifier) to handle the new class. In other words, the model’s capacity can grow for new categories, but we do **not** need to revisit or alter the representation of old classes significantly. The new conv filter will learn to project features of the new class into the existing shallow space (or an expanded space) such that the classifier can recognize it. Because each class’s features are somewhat separated in CLIP’s space already, this addition can be done with minimal interference.
  - We may also use a few stored compressed prototypes of old classes to **rehearse** and ensure the classifier doesn’t forget the old classes while integrating the new one (a common practice in class-incremental learning). The difference here is we use compressed prototypes instead of many raw images, saving memory.

## Experiment Setup

### Dataset

We will validate on an image classification task. A suitable dataset is **COCO**, which contains diverse objects and could be adapted for multi-class classification. We will convert COCO into a classification setting by using its labels (e.g. treat each image as belonging to one primary object category, or create one-vs-all classification tasks for simplicity). Alternatively, we can use a subset of COCO classes and gather images where that class is prominently featured. The choice of COCO is mainly to have a large number of classes and images, which is good for testing incremental addition of classes. (If COCO proves complex for classification, we could fall back to a simpler dataset like CIFAR-100 or ImageNet subset, but COCO is preferred to align with a challenging, large-scale scenario.)

### Model and Training Configuration

- **Feature Extractor:** CLIP ViT-B/16 or ResNet-50 model (pre-trained on image-text data). We obtain for each training image its feature representation from CLIP. This is done once in a preprocessing step to speed up training (since CLIP is frozen, we can cache the features).
- **Compression Mapping:** A single 1×1 convolution layer (with bias) applied on CLIP’s feature map. Suppose CLIP (ResNet-50) gives a 7×7×1024 feature map per image; our conv layer might output a 7×7×`n` feature map. We will experiment with different `n` (e.g. `n = 10, 20, 50`). If using CLIP’s final embedding (1×1×512 per image), the mapping could be a fully connected layer reducing 512 to `n`. The parameters of this conv/layer are initialized randomly and learned via gradient descent.
- **Classifier:** After the conv, we apply global average pooling (so each image is now represented by an `n`-dim feature vector). Then a fully-connected layer maps this `n`-dim vector to class scores (dimension = number of classes). This linear layer is also learned during training. We use softmax and cross-entropy loss for classification.
- **Training Regime:**
  - *Initial training:* Train on an initial subset of classes (e.g. 10 classes from COCO) using their full training images. The CLIP features are inputs; we optimize the conv and classifier parameters. We will observe how many epochs are needed to reach good performance. We expect it to converge rapidly (potentially within 1-2 epochs) because CLIP features are very informative and the model capacity is small. We’ll compare this with a baseline that trains a classifier on top of CLIP without the compression (e.g. a linear probe on CLIP features, which is essentially equivalent to our setup with `n = 512` or disabling the conv compression).
  - *Adding new class:* After initial training, we simulate an incremental learning scenario by adding, say, 1 new class (e.g. an 11th object category from COCO that was not in initial training). We **do not retrain from scratch** on all 11 classes. Instead:
    1. **Proposed incremental update:** We append one new output neuron in the classifier for the new class. We add a small number of new filters in the conv mapping if needed (for example, if we had allocated `n` features for old classes, we might allow some additional channels for the new class’s features). We then train **only on the new class’s data**, plus a few compressed prototypes of old classes for stability. The training for the new class can be just a few gradient steps or one epoch, since we want to evaluate fast adaptation. Because CLIP’s base features already position the new class relative to old ones, and our conv mapping is mostly frozen (except new filters), the classifier should quickly adjust to incorporate the new class. We will measure the new class accuracy and the retention of old classes’ accuracy (catastrophic forgetting test).
    2. **Baseline incremental update:** For comparison, we consider a few baselines:
       - Fine-tune the previously trained model (conv + classifier) on a mix of old and new data (with old data either fully used or limited exemplars). This simulates standard incremental fine-tuning with memory replay. We measure the time/epochs needed and the performance.
       - Train a fresh model from scratch on all 11 classes (upper bound performance, but worst in terms of cost).
       - Additionally, if available, compare with a **dataset distillation baseline**: compress the new class’s data using an existing method (e.g. generate a single synthetic image or gradient-matched prototype for the new class) and see how adding that performs. This may be complex to implement, so a simpler baseline is using **exemplar images**: e.g. choose 1-5 representative images of the new class (perhaps via clustering or just random selection) and fine-tune using those plus exemplars of old classes. This tests whether our learned compression offers an advantage over just picking a few real images.

### Baselines for Comparison

To validate the effectiveness of the proposed method, we will compare against:

- **Full Data Training (Upper Bound):** A classifier trained on top of CLIP features using the entire original training set (no compression). This represents the best achievable accuracy if we had all data and time. It also answers: how much accuracy do we lose, if any, by compressing data? If our method approaches this, it’s successful.
- **Linear Probe on CLIP:** This is essentially training just a linear layer on CLIP features for the initial classes (which our method is very similar to, except with dimensionality reduction). It serves to show how quickly CLIP-based models can learn and sets a baseline for needed epochs. We expect our compressed feature model to need similar or slightly more epochs if `n` is much smaller than the original CLIP dim.
- **Dataset Distillation Methods:** If feasible, implement or reference a known data distillation method (e.g. Zhao et al.’s gradient matching[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=Dataset distillation was first introduced,45%2C 20 %2C  47) or subsequent improvements) on the same initial classes. For instance, use their algorithm to synthesize a small set of images per class (say 1 or 10 per class) and then train a classifier on those. Compare accuracy and training time to our feature-based compression. Many distillation methods are computationally heavy[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=However%2C these methods are compute,Compared to), so we might use results from their papers for comparison if implementation is too complex. The expectation is our simple conv approach is faster/easier, though it may sacrifice some accuracy if extremely compressed.
- **Few-Shot Exemplars:** Compare with a scenario of simply using a few real images per class to train a classifier (a form of manual data reduction). For example, take 5 random images of each class (matching the number of compressed features we use) and train a classifier on that subset. This will likely perform worse than using the entire data, but it’s a baseline for how well naive data reduction works. Our method should outperform this because our compressed features are optimized to be information-dense, not just randomly chosen.
- **Incremental Learning Baseline:** As mentioned, a standard incremental learning approach: when adding a class, fine-tune the model with some stored old data. We can use 5 old images per class (or the same prototypes we saved) and fine-tune the classifier. We will monitor if our method (which primarily updates only new class parameters) retains performance on old classes better than naive fine-tuning (which might overfit to new class or require careful balancing[cs.jhu.edu](https://www.cs.jhu.edu/~yyliu/preprints/Class_Incremental_Exemplar_Compression_for_Class_Incremental_Learning.pdf#:~:text=Class,18%2C 37%2C 42%2C 45%2C 46)).

## Evaluation Metrics

- **Classification Accuracy:** Top-1 accuracy on a held-out test set (or validation set) for all experiments. We will have accuracy for initial classes and, after incremental update, accuracy for old and new classes separately to see if any forgetting occurred.
- **Training Time / Epochs:** We will record how many epochs or iterations are needed for the model to converge or reach a certain accuracy. We expect the compressed feature model to converge in very few epochs. For example, if full data training takes N epochs to reach X% accuracy, our method might reach similar X% in ~1-2 epochs (hypothesis). This will be quantitatively measured by time or epoch count to a threshold accuracy.
- **Model Size and Data Size:** We will quantify the compression achieved. For instance, original CLIP features for all training images vs. compressed prototypes: how much storage is saved? If each class is compressed into, say, 5 prototype feature maps of size n, that’s significantly smaller than storing all images or even all features. We can report the ratio of compression. Also, number of parameters added per new class (to show scalability of incremental addition).
- **Incremental Learning Performance:** Using metrics from incremental learning literature, we will check the **accuracy drop on old classes** after adding new classes (to see if our method avoids catastrophic forgetting). Ideally, using our stored prototypes and frozen mapping, the old class accuracy should remain almost unchanged, demonstrating that new learning did not interfere much. We’ll compare this to the baseline fine-tuning where forgetting might be higher.

## Feasibility Analysis

**Why this should work:** CLIP’s feature space is known to be very general and linearly separable for many concepts (as evidenced by its strong zero-shot and few-shot performance). Training a linear or shallow model on CLIP features usually yields good accuracy with minimal data and epochs. By adding a learnable 1×1 conv on top, we allow a slight adaptation to the specific dataset: essentially, we **extract the most salient features for the task**. This is akin to how a shallow layer of a neural network picks up class-specific activations. The conv compression might incur some information loss (especially if `n` is much smaller than original feature dimensions), but if chosen properly, it forces the model to discard redundant features and keep only those crucial for distinguishing classes – i.e., it creates a *high information density* representation per class, as hypothesized. This aligns with the spirit of dataset distillation: a distilled dataset should have higher info density than raw data[arxiv.org](https://arxiv.org/html/2506.23580v1#:~:text=capacity ,26%2C 2 %2C  28). Here, instead of synthetic images, we produce **synthetic features**.

**Incremental learning advantage:** Because CLIP features of different classes are separable to begin with, our compressed prototypes for each class will likely occupy distinct regions in the shallow feature space. Adding a new class by learning a new prototype (or conv filter) should not drastically alter the existing ones. This modularity means we can expand the model incrementally. Traditional dataset distillation methods have not demonstrated this – they typically compress a fixed set of classes and would need to redo distillation if classes change. By contrast, our approach can *append* new distilled data for new classes on the fly, addressing the critique that “if I already trained on original data, why compress and train again?”: the answer is we often can’t or won’t retrain on all original data for new classes (due to time or memory constraints), and that’s where having a compressed representation shines. It provides a way to quickly **learn new classes with minimal data** while not requiring full retraining or full data storage[cs.jhu.edu](https://www.cs.jhu.edu/~yyliu/preprints/Class_Incremental_Exemplar_Compression_for_Class_Incremental_Learning.pdf#:~:text=Class,18%2C 37%2C 42%2C 45%2C 46). Additionally, the compressed features for old classes can act as a compact memory to prevent forgetting, similar to exemplars but much smaller in size.

**Potential Challenges:**

- Finding the right dimension `n` for the compressed space is important. Too small and we might lose accuracy; too large and we lose the benefit of compression. We will experiment with this.
- The approach assumes CLIP features are good enough that a linear separation exists for the classes. If the chosen dataset/classes are very fine-grained or not well represented in CLIP’s pre-training, the performance might suffer. In such cases, we might need to allow fine-tuning of CLIP or more complex mapping, but that introduces more cost.
- For incremental training, one must ensure the new class’s features actually get represented. We might need to temporarily allow the conv mapping to adjust slightly for the new class (or at least train the new filter well). Also, ensuring the classifier’s weights for old classes remain stable (perhaps by freezing them or using a small learning rate on them during incremental update) will be considered to avoid drifting.

## Experiment Steps Summary (for Implementation)

1. **Data Preparation:** Load COCO dataset. Create a classification task from it (e.g., select 10 classes and gather all images predominantly containing each class). Split into train and test sets.
2. **Feature Extraction with CLIP:** Use a pre-trained CLIP model to extract features for all images. Save these features for reuse.
3. **Baseline Training (No Compression):** Train a simple classifier on CLIP features for the initial classes. Record accuracy and epochs to converge.
4. **Proposed Method – Initial Training:** Initialize the 1×1 conv mapping (with output dim = `n`) and classifier. Train them on the initial classes’ CLIP features. Use early-stopping or a small fixed number of epochs (we expect not many are needed). Track performance.
5. **Compression Effect Analysis:** Compare the performance of the compressed model to the baseline. Also, examine the compressed features:
   - For each class, you can look at the conv output activations or even visualize what each conv filter is focusing on (e.g., by projecting them back through CLIP’s decoder if possible, or checking which images maximize a given output channel). This will give insight into whether each channel corresponds to some prototype or discriminative aspect.
   - Measure the storage: size of all original features vs. size of stored prototypes (if we consider storing, say, the mean activation per class or a few per class). This quantifies compression.
6. **Incremental Learning – Add New Class:** Take a new class not seen before.
   - **Proposed:** Add new conv filter(s) and classifier output. Train on new class’s CLIP features for a few iterations. Optionally include a few prototype features from old classes in the training batch to stabilize (replay). Evaluate on new class and old classes.
   - **Baseline fine-tune:** Starting from the model before addition, fine-tune it with new class data (and maybe some old data) for a comparable number of epochs. Evaluate forgetting and new accuracy.
   - **Baseline from scratch:** (For reference) train a new model on all classes (old+new) together, to see the ideal performance if all data were available.
7. **Compare Incremental Results:** Report the accuracy drop on old classes and accuracy on the new class for both the proposed incremental update and baseline fine-tuning. A successful outcome is that our method achieves comparable new-class accuracy with much less drop on old classes (thanks to minimal interference), and does so with less computational effort (since we didn’t retrain everything).
8. **Repeat for Multiple Increments (Optional):** If time permits, simulate adding multiple classes sequentially (class 11, then 12, 13, etc.), using our method each time. Monitor how the compressed feature space scales – e.g., does the required `n` grow linearly with classes, or can features be shared? This will test the scalability of the approach.
9. **Few-Shot Learning Tests (Optional):** As an additional experiment, test the model’s ability in extreme data-scarce situations. For example, try training the system with only a handful of images per class from the start (this would mimic few-shot learning). CLIP’s features should still allow good performance. Compare this scenario with using our compressed prototypes (which essentially are also a form of synthesized few-shot data). This is to connect with few-shot learning literature and show that our approach can serve as an effective few-shot learner (since it’s basically learning a prototype per class).

Throughout the experiments, we will document the results in detail. If the idea succeeds, we expect to see that a model trained on the **compressed feature space**:

- Reaches near-original accuracy with significantly less training cost (epochs or data) for initial training.
- Can **absorb new classes quickly** without retraining on all old data, while preserving past knowledge.
- Maintains a compact representation (e.g., one conv filter or a few prototypes per class) which could be reused or analyzed (like *“an information capsule for each class”*).

Finally, we will compile these findings to support (or refute) the hypothesis that *“compressing data at the feature level using a strong pre-trained model can yield a highly generalizable shallow feature space, enabling efficient learning and incremental updates.”*